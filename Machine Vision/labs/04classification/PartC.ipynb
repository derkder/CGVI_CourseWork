{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Part C: Fitting the logistic regression model\n",
    "The goal of this part of the practical is to implement Newton's method for fitting logistic regression with 2D data.\n",
    "\n",
    "As for the last part, we have implemented the main function, so you should implement the first and second derivatives. This is much the same as for the last part, however rather than having a fixed function (the Rosenbrock function) the function we are optimising is the negative log-likelihood of the world states `w` given the measurements `x` and the model parameters $\\phi$. Since the data are known and we are only fitting the model parameters, these are fixed before passing the function to the optimiser.\n",
    "\n",
    "**TODO:** When you have this working, compare the number of iterations for steepest descent vs. Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('machinevision.mplstyle')\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from functions import LogisticRegressionNLL\n",
    "from optimisation import NewtonMethod, SteepestDescent, optimise\n",
    "from utils import add_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct data\n",
    "There are two classes of data, each from a 2D Gaussian. The measurements are the coordinates of the data and the world states are the binary class labels. These are concatenated to form a single set of data values.\n",
    "\n",
    "**Note:** since the data are random samples, it can help to set a constant random seed whilst implementing so that the data is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "num_data = 20\n",
    "\n",
    "data1 = np.random.randn(2, num_data) * 0.15 + 0.65\n",
    "label1 = np.ones((1, num_data))\n",
    "\n",
    "data2 = np.random.randn(2, num_data) * 0.19 + 0.4\n",
    "label2 = np.zeros((1, num_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = add_bias(np.concatenate([data1, data2], axis=1))\n",
    "w = np.concatenate([label1, label2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform optimisation\n",
    "This is the same as in [Part B](PartB.ipynb#Perform-optimisation), however the function being optimised is for the maximum likelihood estimation of the logistic regression parameters. \n",
    "\n",
    "The logistic regression negative log-likelihood function defined in [functions.py](functions.py) is optimal when the probability $Pr(w \\vert X,\\phi)$ is maximised.\n",
    "\n",
    "**TODO:** Compute by hand the first and second derivatives for the negative log-likelihood of the logistic regression model (given measurements `x` and world states `w`) and implement them in the `LogisticRegressionNLL` class in [functions.py](functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial = np.array([[1, -2, 2]]).T\n",
    "phi_opt, w_opt = optimise(start_position=initial, tolerance=1e-5, function=LogisticRegressionNLL(x, w),\n",
    "                          optimiser=NewtonMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "For a grid of $x_1$ and $x_2$ values, infer the likelihood of each class according to the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_mesh, x2_mesh = np.mgrid[-0.2:1.2:0.01, -0.2:1.2:0.01]\n",
    "x_domain = add_bias(np.stack([x1_mesh.flatten(), x2_mesh.flatten()]))\n",
    "\n",
    "phi = phi_opt[-1]  # use the phi value from the final (optimal) iteration\n",
    "prob = sigmoid(phi.T @ x_domain).reshape(x1_mesh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "ax.plot(data1[0], data1[1], 'ko', fillstyle='full', label='data 1')\n",
    "ax.plot(data2[0], data2[1], 'ro', fillstyle='full', label='data 2')\n",
    "domain = ax.imshow(prob, origin='lower', extent=[-0.2, 1.2, -0.2, 1.2], cmap='Blues')\n",
    "\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', title='Probability of world state 1')\n",
    "fig.colorbar(domain, fraction=0.046, pad=0.04)\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
