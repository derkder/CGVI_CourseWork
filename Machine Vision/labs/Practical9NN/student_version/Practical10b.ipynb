{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Vision Neural Network tutorial---Part 2\n",
    "Author: Daniel E. Worrall, 3 Dec 2016\n",
    "\n",
    "You are going to write a script to run a 7-layer autoencoder. We have\n",
    "supplied the structure and the pre-trained weights for the autoencoder to\n",
    "run out-of-the-box. The model is:\n",
    "\n",
    "input --> encoder --> latent_code --> decoder --> reconstruction\n",
    "\n",
    "Start by running this script and see what the output gives. You should see that you can generate images that look like numbers by generating random latent codes from a 500D standard Gaussian and passing these vectors through the decoder. Your task will be to find the subspace of the latent code, such that you can smoothly interpolate between numbers in latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and add files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(all='ignore') # Ignore overflows\n",
    "from scipy.io import loadmat\n",
    "from mlp import mlp_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "mnist_images = loadmat('mnist_test')\n",
    "mnist_images = mnist_images['X']\n",
    "\n",
    "# Load params\n",
    "weights = loadmat('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MLP\n",
    "Construct the network as an ordered cell array, where each element is alayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import neural network architecture and plotting functions\n",
    "# No need to read below here (unless you're keen)\n",
    "from utils import build_encoder, build_decoder, plot_tiled_array\n",
    "\n",
    "encoder = build_encoder(weights)\n",
    "decoder = build_decoder(weights)\n",
    "\n",
    "## Inference\n",
    "# Forward\n",
    "n_samples = 225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TODO: Plot some input data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(6,6))\n",
    "plot_tiled_array(mnist_images[:n_samples,:], 'MNIST examples')\n",
    "plt.title('MNIST examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) TODO: Generate 225 random codes as a draw from a 500D standard Gaussian. \n",
    "\n",
    "You should see that the decoder is able to produce convincing images of handwritten digits from random Gaussian draws. What happens if you increase the variance of the draws, by say a factor of 10? Why does this happen?\n",
    "\n",
    "How are these different from Part 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_code = np.zeros(n_samples,500);\n",
    "reconstruction, __ = mlp_forward(decoder, latent_code)\n",
    "\n",
    "plt.figure(1, figsize=(6,6))\n",
    "plot_tiled_array(reconstruction, 'Decoded randomly generated latent codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, you are going to build a linear approximation to the data-manifold in the latent space of the autoencoder. When you walk along this manifold, you will be able to smoothly interpolate between digits, effectively enforcing a smooth ordering on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sampling grid in a 2D subspace\n",
    "lim = 3\n",
    "image_dims =int(np.ceil(np.sqrt(n_samples)))\n",
    "lin_range = np.linspace(-lim,lim,image_dims)\n",
    "X, Y = np.meshgrid(lin_range, lin_range)\n",
    "sampling_grid = np.concatenate([np.reshape(Y,[n_samples,1]), np.reshape(X,[n_samples,1])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) TODO: Generate a random 500D subspace with 2 degrees of freedom.\n",
    "You can do this by generating two random vectors. See what happens when you run this several times. Are all the images you generate valid digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace = np.zeros(2,500)\n",
    "latent_code = sampling_grid@subspace\n",
    "\n",
    "# Reconstruct images from code\n",
    "reconstruction, __ = mlp_forward(decoder, latent_code)\n",
    "\n",
    "plt.figure(3, figsize=(6,6))\n",
    "plot_tiled_array(reconstruction, 'Random subspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now forward pass `mnist_images` through the encoder so that you have a collection of latent data points. Run PCA on the latent codes and keep the first two principal directions. Compare the quality of these digits to those of the random subspaces. What do you notice? Do you think a linear manifold is a good approximation to the true data manifold? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_code, __ = mlp_forward(encoder, mnist_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.1) TODO: Compute the covariance matrix of the latent codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_cov = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) TODO: Do PCA on the sampled code\n",
    "Perform the SVD on the covariance matrix and retain the first two rows of V (as a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_subspace = np.zeros(2, 500)\n",
    "\n",
    "latent_grid = sampling_grid@principal_subspace\n",
    "# Reconstruct images from the codes\n",
    "reconstruction, __ = mlp_forward(decoder, latent_grid)\n",
    "\n",
    "plt.figure(4, figsize=(6,6))\n",
    "plot_tiled_array(reconstruction)\n",
    "plt.title('Fitted subspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL EXTENSIONS\n",
    "In this section you may use whatever Python functionality you see fit to use.\n",
    "\n",
    "_OPTIONAL i_ **(Easy)**:\n",
    "What happens if you pass the reconstruction from the decoder back into the encoder? What happens if you do this T times? Try adding a little isotropic Gaussian noise to the latent code every time you do this. What does this do?\n",
    "\n",
    "_OPTIONAL ii_ **(Moderate)**: Moderate\n",
    "Run a _k-means clustering algorithm_ in the latent space. What do you find?\n",
    "\n",
    "_OPTIONAL iii_ **(Difficult)**:\n",
    "Port some new data to the script, say the Frey Faces dataset (for example, from [here](http://www.cs.nyu.edu/~roweis/data/frey_rawface.mat)). Now replace the final sigmoid layer in the decoder and try to train an encoder yourself on this.\n",
    "\n",
    "_OPTIONAL iv_ **(Difficult)**\n",
    "Read [Autoencoding Variational Bayes (Kingma et al., 2014)](https://arxiv.org/abs/1312.6114). The pretrained weights for this autoencoder were trained using this setup. If you look carefully at ```weights.mat```, you will see that there is an extra set of weights for the encoder ```encoder_W_e_sigma```, which we do not use. Can you implement a stochastic encoder layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
